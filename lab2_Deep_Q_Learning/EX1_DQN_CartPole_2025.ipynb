{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYu0OlZ6sK7d",
        "outputId": "be7e79b0-0c25-4a77-9383-7a73517cf4c3",
        "is_executing": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/958.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/958.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.8/958.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m952.3/958.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n"
      ],
      "metadata": {
        "id": "VpsHzyWmseVk",
        "is_executing": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "cQJgkAMMsEoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model\n",
        "class QNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, state_shape, h1_nodes, action_space_size, seed):\n",
        "        \"\"\" state_shape (int): Dimension of each state\n",
        "            h1_nodes (int): Number of nodes in first hidden layer\n",
        "            action_space_size (int): Dimension of each action\n",
        "            seed (int): Random seed\n",
        "        \"\"\"\n",
        "        super(QNetwork, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        ## add three fully connected layers using nn.Linear()\n",
        "        #-----------------add code here-----------------------\n",
        "        nn.Linear(state_shape)\n",
        "        nn.Linear(h1_nodes)\n",
        "        nn.Linear(action_space_size)\n",
        "\n",
        "\n",
        "    def forward(self, state):\n",
        "      # define the forward propagation  by adding the relu() activation function between layers\n",
        "      #-----------------add code here-----------------------\n",
        "      x = nn.ReLU(state)\n",
        "      x = nn.ReLU(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "jQk7LBu5s3yo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define memory for Experience Replay\n",
        "class ReplayMemory():\n",
        "    def __init__(self, maxlen, batch_size):\n",
        "        self.memory = deque([], maxlen=maxlen)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.memory.append(experience)\n",
        "\n",
        "    def sample(self):\n",
        "        #Randomly sample a batch of experiences from memory\n",
        "        #-----------------add code here-----------------------\n",
        "        return random.choice(self.memory, k=self.batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "W2a1w61qtpWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the Deep QLearning Agent\n",
        "class DQAgent():\n",
        "        \"\"\"Interacts with and learns from the environment.\"\"\"\n",
        "\n",
        "        def __init__(self, state_shape, action_space_size, seed, h1_nodes=64, learning_rate=5e-4, replay_memory_size=int(1e5),batch_size=64, UPDATE_EVERY=20, LEARN_EVERY=4,GAMMA=0.99):\n",
        "           \"\"\"Initialize an Agent object. \"\"\"\n",
        "\n",
        "           self.state_shape = state_shape\n",
        "           self.action_space_size = action_space_size\n",
        "           self.seed = random.seed(seed)\n",
        "           self.GAMMA = GAMMA\n",
        "           self.UPDATE_EVERY = UPDATE_EVERY\n",
        "           self.LERN_EVERY = LEARN_EVERY\n",
        "           self.h1_nodes = h1_nodes\n",
        "           self.learning_rate = learning_rate\n",
        "           self.batch_size = batch_size\n",
        "           self.replay_memory_size = replay_memory_size\n",
        "\n",
        "\n",
        "           # Q-Network\n",
        "           # create the local and the target networks\n",
        "           self.local_dqn = QNetwork(self.state_shape, self.h1_nodes, self.action_space_size, self.seed) #--add code here---\n",
        "           self.target_dqn = QNetwork(self.state_shape, self.h1_nodes, self.action_space_size, self.seed) #--add code here---\n",
        "\n",
        "           self.optimizer = optim.Adam(self.local_dqn.parameters(), lr=self.learning_rate)\n",
        "\n",
        "           # Replay memory\n",
        "           self.memory = ReplayMemory(self.replay_memory_size, self.batch_size)\n",
        "\n",
        "           # Initialize time step (for learn every LEARN_EVERY steps)\n",
        "           self.l_step = 0\n",
        "\n",
        "           # Initialize time step (for updating every UPDATE_EVERY steps)\n",
        "           self.t_step = 0\n",
        "\n",
        "        def state_to_dqn_input(self, state)->torch.Tensor:\n",
        "           return torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "\n",
        "        #Returns actions for given state\n",
        "        #Epsilon-greedy action selection\n",
        "        def act(self, state, eps=0.):\n",
        "          if random.random() > eps:\n",
        "            action_values = self.local_dqn(self.state_to_dqn_input(state))\n",
        "            return np.argmax(action_values.cpu().data.numpy()) #--add code here---\n",
        "          else:\n",
        "            return random.choice(np.arrange(self.action_space_size)) #--add code here---\n",
        "\n",
        "        def step(self, state, action, reward, next_state, done):\n",
        "           # Save experience in replay memory\n",
        "           #----------add code here---\n",
        "\n",
        "           # If enough samples are available in memory, get random subset and learn\n",
        "           self.l_step = (self.l_step + 1) % self.LERN_EVERY\n",
        "           if self.l_step == 0 and len(self.memory) > self.batch_size:\n",
        "                experiences = #--add code here---\n",
        "                #--add code here---\n",
        "\n",
        "           # Copy local network to target network every UPDATE_EVERY time steps\n",
        "           self.t_step = (self.t_step + 1) % self.UPDATE_EVERY\n",
        "           if self.t_step == 0:\n",
        "              #--add code here---\n",
        "\n",
        "        def learn(self, experiences):\n",
        "          q_target_list = []\n",
        "          q_expected_list = []\n",
        "          # Obtain random minibatch of tuples from ReplayMemory\n",
        "          for  state, action, reward, next_state, done in experiences:\n",
        "\n",
        "             # Convert state, next_state, action abd reward to tensors\n",
        "             state =#--add code here---\n",
        "             next_state = #--add code here---\n",
        "             action= torch.from_numpy(np.array([action])).long().unsqueeze(0).to(device)\n",
        "             reward = torch.from_numpy(np.array([reward])).float().to(device)\n",
        "             ## Compute and minimize the loss\n",
        "\n",
        "             ### Extract next maximum estimated value from target network\n",
        "             if(done):\n",
        "               q_target =  #--add code here---\n",
        "             else:\n",
        "               q_target=  #--add code here---\n",
        "\n",
        "             ### Calculate expected value from local network\n",
        "             q_local = #--add code here---\n",
        "\n",
        "             q_target_list.append(q_target)\n",
        "             q_expected_list.append(q_local)\n",
        "\n",
        "          ### Loss calculation (we used Mean squared error)\n",
        "          loss = F.mse_loss(torch.cat(q_expected_list), torch.cat(q_target_list))\n",
        "          self.optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          self.optimizer.step()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YbRg16A-uGjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "def dqn_train(agent, n_episodes=300, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
        "\n",
        "    scores = []                        # list containing scores from each episode\n",
        "    scores_window = deque(maxlen=100)  # last 100 scores\n",
        "    eps = eps_start                    # initialize epsilon\n",
        "\n",
        "    for i_episode in  tqdm(range(1, n_episodes+1)):\n",
        "        state = env.reset()[0]\n",
        "\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            # select an action\n",
        "            action = #--add code here---\n",
        "\n",
        "            #apply the selected action\n",
        "            next_state, reward, done, _,_=env.step(action)\n",
        "\n",
        "            # store the current experience\n",
        "            #--add code here---\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores_window.append(score)       # save most recent score\n",
        "        scores.append(score)              # save most recent score\n",
        "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
        "\n",
        "        if i_episode % 50 == 0:\n",
        "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "\n",
        "    torch.save(agent.local_dqn.state_dict(), 'checkpoint.pth')\n",
        "    return scores"
      ],
      "metadata": {
        "id": "-sVzEgJ4Wzes"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('CartPole-v1')\n",
        "print(env.reset())\n",
        "\n",
        "print('State shape: ', env.observation_space.shape[0])\n",
        "print('Number of actions: ', env.action_space.n)"
      ],
      "metadata": {
        "id": "dgqM0KQY8474"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = DQAgent(state_shape=env.observation_space.shape[0], action_space_size=env.action_space.n, seed=0)\n",
        "scores = dqn_train(agent)"
      ],
      "metadata": {
        "id": "GQ53wv7a-fEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = pd.Series(scores, name=\"scores\")\n",
        "scores.describe()\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "_ = scores.plot(ax=ax, label=\"Scores\")\n",
        "_ = (scores.rolling(window=100)\n",
        "           .mean()\n",
        "           .rename(\"Rolling Average\")\n",
        "           .plot(ax=ax))\n",
        "ax.legend()\n",
        "_ = ax.set_xlabel(\"Episode Number\")\n",
        "_ = ax.set_ylabel(\"Score\")"
      ],
      "metadata": {
        "id": "u3kPJmXM4Ss4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}