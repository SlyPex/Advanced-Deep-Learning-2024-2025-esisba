{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "T6IMu0_rY_h7"
      },
      "source": [
        "# Actor-Critic Algorithm\n",
        "In this notebook, you'll code The Actor-Critic Algorithm from scratch: .\n",
        "\n",
        "Actor-Critic algorithm is a *Policy-based method* that aims to reduce the variance of the Reinforce algorithm and train our agent faster and better by using a combination of Policy-Based and Value-Based methods\n",
        "\n",
        "\n",
        "To test its robustness, we're going to train it in Cartpole-v1 environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "P9M8KjAzY_iE"
      },
      "source": [
        "## Import the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qH14nvkuY_iF",
        "outputId": "3be8d56a-92f9-47d8-fe2e-7ea261c2a0b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.3.0)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.3.0)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "J3NZuPhXY_iI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Gym\n",
        "import gymnasium as gym\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "LP-Mrz0QY_iJ"
      },
      "source": [
        "## Check if we have a GPU\n",
        "- Let's check if we have a GPU `device:cuda0`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "df1cJ9_xY_iK"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "TXEeysDIY_iL"
      },
      "source": [
        "# Agent: Playing CartPole-v1 ðŸ¤–\n",
        "### The CartPole-v1 environment\n",
        "\n",
        "> A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart.\n",
        "\n",
        "So, we start with CartPole-v1. The goal is to push the cart left or right **so that the pole stays in the equilibrium.**\n",
        "\n",
        "The episode ends if:\n",
        "- The pole Angle is greater than Â±12Â°\n",
        "- Cart Position is greater than Â±2.4\n",
        "- Episode length is greater than 500\n",
        "\n",
        "We get a reward ðŸ’° of +1 every timestep the Pole stays in the equilibrium."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "lcwtErdjY_iN"
      },
      "outputs": [],
      "source": [
        "#env_id = \"CartPole-v1\"\n",
        "env_id=\"LunarLander-v3\"\n",
        "# Create the env\n",
        "env = gym.make(env_id)\n",
        "\n",
        "\n",
        "# Get the state space and action space\n",
        "s_size = env.observation_space.shape[0]\n",
        "a_size = env.action_space.n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "5QM-DXucY_iQ"
      },
      "source": [
        "## Let's build the A2C algo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "RO2cWL8CY_iS"
      },
      "outputs": [],
      "source": [
        "#Using a neural network to learn our actor (policy) parameters\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(s_size, h_size)\n",
        "        self.fc2 = nn.Linear(h_size, a_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "    def act(self, state):\n",
        "\n",
        "        probs = self.forward(state)\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Q97gPStiY_iT"
      },
      "outputs": [],
      "source": [
        "#Using a neural network to learn state value\n",
        "class Critic(nn.Module):\n",
        "\n",
        "    #Takes in state\n",
        "    def __init__(self, s_size, h_size):\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        # two fully connected layers\n",
        "        self.input_layer = nn.Linear(s_size, h_size)\n",
        "        self.output_layer = nn.Linear(h_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #input layer\n",
        "        x = self.input_layer(x)\n",
        "\n",
        "        #activiation relu\n",
        "        x = F.relu(x)\n",
        "\n",
        "        #get state value\n",
        "        state_value = self.output_layer(x)\n",
        "\n",
        "        return state_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "J3_801bDY_iU"
      },
      "source": [
        "# Building the parts of our algorithm #\n",
        "The main steps for building a A2C Algo are:\n",
        "1. Generates a trajectory\n",
        "2. Compute the discounted returns\n",
        "3. Standardization of the returns\n",
        "4. Train critic network\n",
        "5. Train actor network\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "_sCMBtKcY_iU"
      },
      "outputs": [],
      "source": [
        "def generate_trajectory(actor, critic, max_t):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "        state_values=[]\n",
        "\n",
        "        state, info = env.reset()\n",
        "        for t in range(max_t):\n",
        "            state=torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "\n",
        "            action, log_prob = actor.act(state)\n",
        "            # get the state value from th critic network\n",
        "            state_val = critic(state)\n",
        "\n",
        "            next_state, reward, done, _ , __ = env.step(action)\n",
        "\n",
        "            # add te obtained results to their relative lists ==> saved_log_probs, rewards, state_values\n",
        "\n",
        "            saved_log_probs.append(log_prob)\n",
        "            state_values.append(state_val)\n",
        "            rewards.append(reward)\n",
        "\n",
        "            state=next_state\n",
        "\n",
        "            if done:\n",
        "\n",
        "                break\n",
        "\n",
        "        return  saved_log_probs, rewards, state_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "TCVcuaiLY_iV"
      },
      "outputs": [],
      "source": [
        "def computer_cumulative_reward(rewards, max_t,gamma):\n",
        "\n",
        "        returns = deque(maxlen=max_t)\n",
        "        n_steps = len(rewards)\n",
        "        for t in range(n_steps)[::-1]:\n",
        "          disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
        "          returns.appendleft( rewards[t]+gamma*disc_return_t)\n",
        "        return returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "6iiONPljY_iV"
      },
      "outputs": [],
      "source": [
        "def returns_standardization(returns):\n",
        "        eps = np.finfo(np.float32).eps.item()\n",
        "        ## eps is the smallest representable float, which is\n",
        "        # added to the standard deviation of the returns to avoid numerical instabilities\n",
        "        returns = torch.tensor(returns).to(device)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "        return returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "PfX_wUqxY_iW"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_actor(actorOptimizer,saved_log_probs, returns,state_values):\n",
        "\n",
        "\n",
        "        state_values= torch.stack(state_values).squeeze()\n",
        "\n",
        "        #calculate Advantage for actor\n",
        "        advantages = returns - state_values.detach()\n",
        "\n",
        "        #convect the advantages to a tensor\n",
        "        advantages = advantages.clone().detach().to(device)\n",
        "\n",
        "        actor_loss = []\n",
        "        # compute the actor loss\n",
        "        for log_prob, advantage in zip(saved_log_probs, advantages):\n",
        "            actor_loss.append(-log_prob * advantage)\n",
        "\n",
        "\n",
        "        actor_loss = torch.cat(actor_loss).sum()\n",
        "        # Backpropagate actor\n",
        "        actorOptimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        actorOptimizer.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "FOqUlT_SY_iW"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_critic(criticOptimizer, returns,state_values):\n",
        "        state_values= torch.stack(state_values).squeeze()\n",
        "\n",
        "        critic_loss = F.mse_loss(state_values, returns.type(torch.float32))\n",
        "\n",
        "        # Backpropagate crtic\n",
        "        criticOptimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        criticOptimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "eOq5306jY_iX"
      },
      "source": [
        "## Merge all functions into the Actor_Critic method ##\n",
        "\n",
        "You will now see how the overall **A2C Algo** is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "LYfTpEHDY_iX"
      },
      "outputs": [],
      "source": [
        "def Actor_Critic(actor,critic, actorOptimizer,criticOptimizer, n_training_episodes, max_t, gamma, print_every):\n",
        "    # Help us to calculate the score during the training\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "\n",
        "    for i_episode in range(1, n_training_episodes+1):\n",
        "\n",
        "        # Generate an episode\n",
        "        saved_log_probs, rewards, state_values = generate_trajectory(actor, critic, max_t)\n",
        "\n",
        "        # Compute the total reward by the end of the episode\n",
        "        scores_deque.append(sum(rewards))\n",
        "        scores.append(sum(rewards))\n",
        "\n",
        "        # calculate the return\n",
        "        returns= computer_cumulative_reward(rewards,max_t,gamma)\n",
        "\n",
        "        ## standardization of the returns is employed to make training more stable\n",
        "        returns=returns_standardization(returns)\n",
        "\n",
        "        # Train the Critic network\n",
        "        train_critic(criticOptimizer,returns,state_values)\n",
        "\n",
        "        # Train the Actor network\n",
        "        train_actor(actorOptimizer,saved_log_probs, returns,state_values)\n",
        "\n",
        "\n",
        "\n",
        "        if i_episode % print_every == 0:\n",
        "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "E8Ax7IZfY_iY"
      },
      "source": [
        "##  Train it\n",
        "- We're now ready to train our agent.\n",
        "- But first, we define a variable containing all the training hyperparameters.\n",
        "- You can change the training parameters (and should ðŸ˜‰)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "nqg89aqzY_iY"
      },
      "outputs": [],
      "source": [
        "cartpole_hyperparameters = {\n",
        "    \"h_size\": 64,\n",
        "    \"n_training_episodes\": 10000,\n",
        "    \"n_evaluation_episodes\": 10,\n",
        "    \"max_t\": 1000,\n",
        "    \"gamma\": 1.0,\n",
        "    \"lr\": 1e-2,\n",
        "    \"env_id\": env_id,\n",
        "    \"state_space\": s_size,\n",
        "    \"action_space\": a_size,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "heKHlDWXY_iY"
      },
      "outputs": [],
      "source": [
        "# Create actor and place it to the device\n",
        "cartpole_actor = Actor(cartpole_hyperparameters[\"state_space\"],cartpole_hyperparameters[\"action_space\"],cartpole_hyperparameters[\"h_size\"]).to(device)\n",
        "\n",
        "cartpole_actorOptimizer = optim.Adam(cartpole_actor.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "EnFDSKwGY_iZ"
      },
      "outputs": [],
      "source": [
        "# Create critic and place it to the device\n",
        "cartpole_critic = Critic(cartpole_hyperparameters[\"state_space\"],cartpole_hyperparameters[\"h_size\"]).to(device)\n",
        "cartpole_criticOptimizer = optim.Adam(cartpole_critic.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "5-LdCOspY_iZ"
      },
      "outputs": [],
      "source": [
        "scores = Actor_Critic(cartpole_actor,\n",
        "                   cartpole_critic,cartpole_actorOptimizer,cartpole_criticOptimizer,\n",
        "                   cartpole_hyperparameters[\"n_training_episodes\"],\n",
        "                   cartpole_hyperparameters[\"max_t\"],\n",
        "                   cartpole_hyperparameters[\"gamma\"],\n",
        "                   100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "QosdOwueY_iZ"
      },
      "outputs": [],
      "source": [
        "scores= pd.Series(scores, name=\"scores_Actor\")\n",
        "scores.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "id": "8M6VB4wWY_ia"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1)\n",
        "_ = scores.plot(ax=ax, label=\"scores_Actor\")\n",
        "_ = (scores.rolling(window=100)\n",
        "           .mean()\n",
        "           .rename(\"Rolling Average\")\n",
        "           .plot(ax=ax))\n",
        "ax.legend()\n",
        "_ = ax.set_xlabel(\"Episode Number\")\n",
        "_ = ax.set_ylabel(\"scores_Actor\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XzOGVlKJjEUT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}